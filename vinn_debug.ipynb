{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mglob\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mh5py\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhydra\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmmap\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'h5py'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import h5py\n",
    "import hydra\n",
    "import mmap\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torch.utils.data as data \n",
    "import random\n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm \n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torchvision import models\n",
    "from holobot.robot.allegro.allegro_kdl import AllegroKDL\n",
    "\n",
    "from utils.constant import *\n",
    "from model.utils import *\n",
    "# from model.custom import *\n",
    "# from datasets.tactile_vision import *\n",
    "# from tactile_learning.deployment.load_models import * \n",
    "from deployer.nnbuffer import NearestNeighborBuffer\n",
    "from model.knneighbor import KNearestNeighbors, ScaledKNearestNeighbors\n",
    "from utils.visualization import *\n",
    "from utils.tactile_image import *\n",
    "from utils.data import load_data, load_dataset_image\n",
    "from torchvision.transforms.functional import crop\n",
    "from holobot.samplers.allegro import AllegroSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = '/scratch/yd2032/Desktop/holobot_data/cube_flipping'\n",
    "TEST_ROOTS = sorted(glob.glob(f'{TEST_DIR}/demonstration_*'))\n",
    "REPR_DIR = '/scratch/yd2032/Desktop/holobot_data/cube_flipping'\n",
    "REPR_ROOTS = sorted(glob.glob(f'{REPR_DIR}/demonstration_*'))\n",
    "\n",
    "#ALEXMNET_TACK_OUT_DIR_NOT_PRETRAINED ??\n",
    "class TINNStarter: \n",
    "    def __init__(\n",
    "        self,\n",
    "        image_encoder_out_dir,\n",
    "        # image_nontrained=False,\n",
    "        view_num = 0,\n",
    "        test_demos = [],\n",
    "        repr_demos =[]\n",
    "    ):\n",
    "        os.enciron[\"MASTER_ADDR\"] = \"localhost\"\n",
    "        os.environ[\"MASTER_PORT\"] = \"29505\"\n",
    "\n",
    "        torch.cuda.set_device(0)\n",
    "        self.device = torch.device('cuda:0')\n",
    "\n",
    "        self.view_num = view_num\n",
    "        self.image_cfg, self.image_encoder, self.image_transform = self._init_encoder_info(device, image_encoder_out_dir) \n",
    "        self.inv_image_transform = self._get_inverse_image_norm()\n",
    "\n",
    "        self.test_data = load_data(TEST_ROOTS, demos_to_use=test_demos)\n",
    "        # print('TEST_DATA LEN: {}'.format(sum(self.test_data['length'])))\n",
    "        self.repr_data = load_data(REPR_ROOTS, demos_to_use=repr_demos)\n",
    "\n",
    "        IMAGE_RPER_SIZE = 512\n",
    "\n",
    "        #get test representations\n",
    "        self.test_repr =   all_representations = np.zeros((\n",
    "                          0, 528\n",
    "                          ))\n",
    "        for demo_id, root in enumerate (TEST_ROOTS):\n",
    "            if demo_id == 0:\n",
    "                print(\"demo id:{}\".format(demo_id))\n",
    "                self.test_sampler = AllegroSampler(root, [0], 'rgb', 0.0005)\n",
    "                self.test_sampler.sample_data()\n",
    "                ##store the image and state information: \n",
    "                for index in range(len(sampler.sampled_robot_states)):\n",
    "                     ## states\n",
    "                    representation = self.test_sampler.sampled_robot_states[index]\n",
    "                    ## image and preproccessing\n",
    "                    dset_img = load_dataset_image(TEST_DIR, demo_id, self.test_sampler.sampled_rgb_frame_idxs[0][index], 0)\n",
    "                    img = torch.FloatTensor(self.image_transform(dset_img)).to(self.device)\n",
    "                    image = self.image_encoder(img.unsqueeze(dim=0)) # Add a dimension to the first axis so that it could be considered as a batch\n",
    "                    image = image.detach().cpu().numpy().squeeze()\n",
    "                    representation = np.concatenate([image, representation], axis=0)\n",
    "                    self.test_repr=np.vstack((self.test_repr, representation))\n",
    "        print(\"---------------------------------------------------------num of frames sampled {}, demo: {}\".format(len(self.test_sampler.sampled_rgb_frame_idxs[0]),demo_id))\n",
    "\n",
    "\n",
    "        #get all representations\n",
    "        self.all_repr =   all_representations = np.zeros((\n",
    "                          0, 528\n",
    "                          ))\n",
    "        for demo_id, root in enumerate (TEST_ROOTS):\n",
    "            if demo_id in [0,1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]:\n",
    "                continue\n",
    "            print(\"demo id:{}\".format(demo_id))\n",
    "            self.sampler = AllegroSampler(root, [0], 'rgb', 0.0005)\n",
    "            self.sampler.sample_data()\n",
    "            ##store the image and state information: \n",
    "            for index in range(len(self.sampler.sampled_robot_states)):\n",
    "                 ## states\n",
    "                representation = self.sampler.sampled_robot_states[index]\n",
    "                ## image and preproccessing\n",
    "                dset_img = load_dataset_image(TEST_DIR, demo_id, self.sampler.sampled_rgb_frame_idxs[0][index], 0)\n",
    "                img = torch.FloatTensor(self.image_transform(dset_img)).to(self.device)\n",
    "                image = self.image_encoder(img.unsqueeze(dim=0)) # Add a dimension to the first axis so that it could be considered as a batch\n",
    "                image = image.detach().cpu().numpy().squeeze()\n",
    "                representation = np.concatenate([image, representation], axis=0)\n",
    "                self.all_repr=np.vstack((self.alll_repr, representation))\n",
    "        print(\"---------------------------------------------------------num of frames sampled {}, demo: {}\".format(len(self.sampler.sampled_rgb_frame_idxs[0]),demo_id))\n",
    "\n",
    "        self.image_knn = ScaledKNearestNeighbors(\n",
    "            self.all_repr[:,:512],\n",
    "            self.all_repr[:,:512],\n",
    "            ['image'],\n",
    "            [1],\n",
    "        )\n",
    "\n",
    "        self.K = 5\n",
    "        self.TEST_NUM = sum(self.test_data['length'])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _init_encoder_info(self, device, out_dir): # encoder_type: either image or tactile\n",
    "        cfg = OmegaConf.load(os.path.join(out_dir, '.hydra/config.yaml'))\n",
    "        model_path = os.path.join(out_dir, 'models/byol_encoder_best.pt')\n",
    "        encoder = load_model(cfg, device, model_path)\n",
    "\n",
    "        transform = T.Compose([\n",
    "                T.Resize((480,640)),\n",
    "                T.Lambda(self._crop_transform),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(VISION_IMAGE_MEANS, VISION_IMAGE_STDS),\n",
    "            ])\n",
    "        \n",
    "        return cfg, encoder, transform\n",
    "    \n",
    "    def _get_inverse_image_norm():\n",
    "        np_means = np.array(VISION_IMAGE_MEANS)\n",
    "        np_stds =np.array(VISION_IMAGE_STDS)\n",
    "\n",
    "        inv_normalization_transform = T.Compose([\n",
    "            T.Normalize(mean= [0,0,0],std = 1/np_stds),\n",
    "            T.Normalize(mean = -np_means, std = [1,1,1])\n",
    "        ])\n",
    "        return inv_normalization_transform\n",
    "\n",
    "    \n",
    "    def _crop_transform(self, image):\n",
    "        if self.view_num == 0:\n",
    "            return crop(image, 0,0,480,480)\n",
    "        elif self.view_num == 1:\n",
    "            return crop(image, 0,120,480,640)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_all_neighbors(self, k):\n",
    "        self.K = k\n",
    "        repr_size = 512\n",
    "        self.all_neighbors = np.zeros((\n",
    "            len(self.test_repr), self.K\n",
    "        )).astype(int)\n",
    "        for repr_id, test_repr in enumerate(self.text_repr):\n",
    "            curr_neighbor_data = self._get_one_neighbor_for_all_types(test_repr)\n",
    "            self.all_neighbots[repr_id,:] = curr_neighbor_data['image']['ids'][:]\n",
    "            self.neighbor_data.append(curr_neighbor_data)\n",
    "            \n",
    "    def _get_one_neighbor_for_all_types(self, test_repr):\n",
    "        _, image_neighbor_ids, image_neighbor_dists = self.image_knn.get_k_nearest_neighbors(test_repr[:512], k=self.K)\n",
    "\n",
    "        neighbors = dict(\n",
    "            image = dict(ids = image_neighbor_ids, dists = image_neighbor_dists)\n",
    "        )\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def plot_all_neighbors(self, repr_types):\n",
    "        if isinstance(repr_types, list):\n",
    "            all_nn_idxs = self.all_neighbors[:,:,2]\n",
    "            repr_type_str = 'all'\n",
    "        elif repr_types == 'image':\n",
    "            all_nn_idxs = self.all_neighbors[:,:]\n",
    "            repr_type_str = 'image'\n",
    "        elif repr_types == 'tactile':\n",
    "            all_nn_idxs = self.all_neighbors[:,:,1]\n",
    "            repr_type_str = 'tactile'\n",
    "\n",
    "        repr_indices = random.choices(range(sum(self.test_data['length'])), k=min(30,sum(self.test_data['length'])))\n",
    "        TEST_NUM = len(range(0,len(repr_indices)))\n",
    "\n",
    "        figsize=((self.K+1)*12, TEST_NUM)\n",
    "        fig, axs = plt.subplots(figsize=figsize, nrows=TEST_NUM, ncols=self.K+1)\n",
    "        axs[0][0].set_title(\"Actual\")\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            axs[0][i+1].set_title(f\"{i+1}th Neighbor\")\n",
    "        \n",
    "        ## plot the representation it self\n",
    "        for axs_id, test_id in enumerate(repr_indices):\n",
    "            test_image_id = self.test_sampler.sampled_rgb_frame_idxs[0][test_id]\n",
    "            image = load_dataset_image(TEST_ROOTS, 0, test_image_id)\n",
    "            img = torch.FloatTensor(self.image_transform(image)).to(self.device)\n",
    "            test_img = self.inv_image_transform(img).numpy().transpose(1,2,0)\n",
    "            test_img_cv2 = test_img*255\n",
    "\n",
    "            self.plot_state(\n",
    "                axs[axs_id][0], test_img_cv2\n",
    "            )\n",
    "            ## plot the corresponding neighbors\n",
    "            for k in range(self.k):\n",
    "                nn_id = all_nn_idxs[test_id, k]\n",
    "                nn_image_id = self.test_sampler.sampled_rgb_frame_idxs[0][nn_id]\n",
    "                image = load_dataset_image(REPR_ROOTS, 0, nn_image_id)\n",
    "                img = torch.FloatTensor(self.image_transform(image)).to(self.device)\n",
    "                nn_img = self.inv_image_transform(img).numpy().transpose(1,2,0)\n",
    "                nn_img_cv2 = nn_img*255\n",
    "                self.plot_state(\n",
    "                    axs[axs_id][k+1], nn_img_cv2\n",
    "                )\n",
    "                axs[axs_id][k+1].set_xlabel('Repr_Types: {} - Dists: {}'.format(repr_types, self.neighbor_data[test_id][repr_type_str]['dists'][k]))\n",
    "\n",
    "        \n",
    "    def plot_state(self, ax, image):\n",
    "        title = 'vinn_debug_dumped'\n",
    "        self._dump_vision_state(None, None, title = title, vision_state = image)\n",
    "        curr_state = cv2.imread(f'{title}.png')\n",
    "        ax.imshow(curr_state)\n",
    "\n",
    "    \n",
    "    def _dump_vision_state(self, allegro_tip_pos, kinova_cart_pos, title='curr_state', vision_state=None):\n",
    "        cv2.imwrite(f'{title}_vision.png', vision_state)\n",
    "        vision_img = cv2.imread(f'{title}_vision.png')\n",
    "        state_img = vision_img\n",
    "        cv2.imwrite(f'{title}.png', state_img)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
